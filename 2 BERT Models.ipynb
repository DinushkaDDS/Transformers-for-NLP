{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Models and Usages\n",
    "\n",
    "Earlier we discussed about the original transformer architecture and what it consists of. We can change the way components inside a transformer interact with each other and build different model. And in the same way we can add new components as well. That is what exactly Bert models do. They have introduced a new component name `Bidirectional multihead attention` sub layer. This helps the new transformer architecture to look at all the words at the same time.\n",
    "\n",
    "BERT or `Bidirectional Encoder Representations from Transformers` only uses blocks from the encoder stack in the original transformer and does not even have a decoder stack. \n",
    "\n",
    "### **Architecture of the BERT model**\n",
    "\n",
    "As I said earlier BERT introduced bidirectional attention to the transformer models. It require many changes to the original Transformer model. Below are the such changes that can be found in BERT models compared to what we discussed in the original transformer.\n",
    "\n",
    "\n",
    "#### BERT Encoder stack\n",
    "The BERT model does not use a decoder layers. It also does the masking of tokens but on the attention layer of the encoder. Also original transformer model had 6 encoder layer stacks with 512 size dimensionality. It's attention layer has 8 heads as well.\n",
    "\n",
    "But in the BERT models it is different.\n",
    "\n",
    "- **BERT base model:** This contained 12 encoder layers with 768 dimensionality. Also attention layer had 12 heads to make each of them get 64 size.\n",
    "\n",
    "- **BERT large model:** This contained 24 encoder layer stack with 1024 dimensionality. Also attention layer had 16 heads to make each of them get 64 size.\n",
    "\n",
    "And due to these large number of dimension sizes and layer counts, BERT models have high number of parameters to train as well compared to the original transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another difference is the masked attention. BERT model researchers found out that having a mask in the attention layer can actually impede the attention process.To mitigate this issue BERT authors came up with new idea. That is pretraining the model with different approach/objective!\n",
    "\n",
    "Practically speaking it means BERT model can be trained with 2 approaches.\n",
    "\n",
    "1. Masked Language Modelling (MLM)\n",
    "2. Next Sentence Prediction (NSP)\n",
    "\n",
    "**Masked Language Modelling**\n",
    "\n",
    "In this task, instead of training a model with sequence of visible words followed by a masked sequence to predict, BERT introduces a bidirectional analysis of a sentence with a random mask on a word of a sentence.\n",
    "\n",
    "> BERT uses a technique called `WordPiece` tokenization method to the input. It also uses learned positional encoding. Not sine/cosine approach as in original transformer.\n",
    "\n",
    "For example take the below sentence.\n",
    "\n",
    "    You cannot pass. I am a servant of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "BERT would mask this like below\n",
    "\n",
    "    You cannot pass. I am a [MASK] of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "\n",
    "With this type of input, multi attention sub layer can now try to predict the masked token. (kinda like word2vec method skipgram)\n",
    "\n",
    "There are some tricks used during training to force the model to train longer while producing better results. \n",
    "\n",
    "- Surprising the model by not masking anything for 10% of the dataset.</br>\n",
    "    `The dark fire will not [avail] you, flame of Udun.`\n",
    "- Surprise the model by replacing the masking token with a random token 10% of time.</br>\n",
    "    `The dark fire will not [dance] you, flame of Udun.`\n",
    "- Rest of the data mask by a special token.</br>\n",
    "    `The dark fire will not [MASK] you, flame of Udun.`\n",
    "\n",
    "These weird approches helped the model to avoid overfitting while making the model training efficient.\n",
    "\n",
    "**Next Sentence Prediction**\n",
    "\n",
    "In this technique, the input privided with 2 sentences and 2 new tokens were added.\n",
    "\n",
    "1. **\\[CLS]** : A binary classification like token that get added to the beginning of a sentence to predict if the second sequence follows the first sequence. (ie: posive sample if two sentences are actually consecutive. If sentences are not consecutive then negative)\n",
    "\n",
    "2. **\\[SEP]** A token to indicate the separation/end of a sentence.\n",
    "\n",
    "Example usage of above would be like below.\n",
    "\n",
    "The input sentence\n",
    "\n",
    "    You cannot pass. I am a servant of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "BERT input\n",
    "\n",
    "    [CLS] You cannot pass [SEP] I am a servant of the Secret Fire, wielder of the flame of Anor [SEP]\n",
    "\n",
    "\n",
    "As we can probably guess this require the sentences to embedded differently to make sure that encoding include addtional information to distiquish 2 sequences. Check the below image extracted from the reference book.\n",
    "\n",
    "<center><image src=\"imgs/11.jpg\" width=\"500\"/></center>\n",
    "\n",
    "BERT input embeddings are obtained by summing up 3 types of embeddings token embeddings, sentence sengment embeddings and positional encodings. In additional precise manner, the process of preparing input embeddings for BERT is as follows.\n",
    "\n",
    "- Sequence of text is broken into tokens using `WordPiece` technique.\n",
    "- A \\[MASK] token will replace a random token for Masked Language Model training.\n",
    "- A \\[CLS] classification token is inserted at the beginning of the sequence for classification purposes.\n",
    "- \\[SEP] token is used to indicate the separation/end of sequences for NSP training.\n",
    "- Sentence embedding is added to token embedding, so that 2 sentence sequences have different embedding values.\n",
    "- Positional encoding is learned through the model.\n",
    "\n",
    "Some other key features of BERT are,\n",
    "\n",
    "- It has both unsupervised and supervised training elements.\n",
    "- Bidirectional attention is used in all the attention layers.\n",
    "\n",
    "> Above only describes very high level differences and mechanisms of the BERT model. It is recommended to read more detailed explanations to see the actual implementation details and complexities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model training framework\n",
    "\n",
    "\n",
    "BERT is a two step framework. It invloves a pretraining step and then a finetuning step.\n",
    "\n",
    "<center><image src=\"imgs/12.png\" width=\"700\"/></center>\n",
    "\n",
    "\n",
    "In the pretraining part, we need to define the model architecture, number of layers, number of heads, dimensions and other things. Then once a base model defined it will get trained with very large amount of data like wikipedia data, Bookcorpus etc using the techniques we mentioned like MLM and NSP.\n",
    "\n",
    "Then the model that got pretrained will be tuned to the required task/objective with more specialized labeled datasets. These downstream specialized task include Natural Language understanding , Question answering and Adversarial Generation Sentence-Pairs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Fine Tuning Example\n",
    "\n",
    "Below we will try to finetune a bert model for a task called Neural Network Acceptability Judgement. TO do that we will use the Corpus of Linguistic Acceptability(CoLA).\n",
    "\n",
    "> Using a GPU is essential, so using google colab is recommended. Anyway code is here use at you own risk!\n",
    "\n",
    "First thing we need to do is checking whether the GPU is accessible to the framework we are using. We can check it like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to install the BERT pretrained model To fo that, we will use the hugging face transformers python package. We can install the package by `pip install transformers` command.\n",
    "\n",
    "Once it is completed we can import the modules we will require for the task. Specially pretrained modules, pretrained BERT tokenizer and configurations of BERT model along with BERTAdam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then since we are trying to use the pytorch based BERT model, we need to specify pytorch to use CUDA. So we need to get the device variable in hand like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to work, related data files are in the data folder. We can observe the data set using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our friends wo n't buy this analysis , let alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one more pseudo generalization and i 'm giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one more pseudo generalization or i 'm giving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the more we study verbs , the crazier they get .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>day by day the facts are getting murkier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>poseidon appears to own a dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>digitize is my happiest memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it is easy to slay the gorgon .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i had the strangest feeling that i knew you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what all did you get for christmas ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8551 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "0               gj04      1         NaN   \n",
       "1               gj04      1         NaN   \n",
       "2               gj04      1         NaN   \n",
       "3               gj04      1         NaN   \n",
       "4               gj04      1         NaN   \n",
       "...              ...    ...         ...   \n",
       "8546            ad03      0           *   \n",
       "8547            ad03      0           *   \n",
       "8548            ad03      1         NaN   \n",
       "8549            ad03      1         NaN   \n",
       "8550            ad03      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "0     our friends wo n't buy this analysis , let alo...  \n",
       "1     one more pseudo generalization and i 'm giving...  \n",
       "2     one more pseudo generalization or i 'm giving ...  \n",
       "3      the more we study verbs , the crazier they get .  \n",
       "4            day by day the facts are getting murkier .  \n",
       "...                                                 ...  \n",
       "8546                   poseidon appears to own a dragon  \n",
       "8547                     digitize is my happiest memory  \n",
       "8548                    it is easy to slay the gorgon .  \n",
       "8549      i had the strangest feeling that i knew you .  \n",
       "8550               what all did you get for christmas ?  \n",
       "\n",
       "[8551 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['sentence_source', 'label', 'label_notes', 'sentence']\n",
    "input_data = pd.read_csv('data/in_domain.tsv', sep='\\t', header=None, names=col_names)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
