{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Models and Usages\n",
    "\n",
    "Earlier we discussed about the original transformer architecture and what it consists of. We can change the way components inside a transformer interact with each other and build different model. And in the same way we can add new components as well. That is what exactly Bert models do. They have introduced a new component name `Bidirectional multihead attention` sub layer. This helps the new transformer architecture to look at all the words at the same time.\n",
    "\n",
    "BERT or `Bidirectional Encoder Representations from Transformers` only uses blocks from the encoder stack in the original transformer and does not even have a decoder stack. \n",
    "\n",
    "### **Architecture of the BERT model**\n",
    "\n",
    "As I said earlier BERT introduced bidirectional attention to the transformer models. It require many changes to the original Transformer model. Below are the such changes that can be found in BERT models compared to what we discussed in the original transformer.\n",
    "\n",
    "\n",
    "#### BERT Encoder stack\n",
    "The BERT model does not use a decoder layers. It also does the masking of tokens but on the attention layer of the encoder. Also original transformer model had 6 encoder layer stacks with 512 size dimensionality. It's attention layer has 8 heads as well.\n",
    "\n",
    "But in the BERT models it is different.\n",
    "\n",
    "- **BERT base model:** This contained 12 encoder layers with 768 dimensionality. Also attention layer had 12 heads to make each of them get 64 size.\n",
    "\n",
    "- **BERT large model:** This contained 24 encoder layer stack with 1024 dimensionality. Also attention layer had 16 heads to make each of them get 64 size.\n",
    "\n",
    "And due to these large number of dimension sizes and layer counts, BERT models have high number of parameters to train as well compared to the original transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another difference is the masked attention. BERT model researchers found out that having a mask in the attention layer can actually impede the attention process.To mitigate this issue BERT authors came up with new idea. That is pretraining the model with different approach/objective!\n",
    "\n",
    "Practically speaking it means BERT model can be trained with 2 approaches.\n",
    "\n",
    "1. Masked Language Modelling (MLM)\n",
    "2. Next Sentence Prediction (NSP)\n",
    "\n",
    "**Masked Language Modelling**\n",
    "\n",
    "In this task, instead of training a model with sequence of visible words followed by a masked sequence to predict, BERT introduces a bidirectional analysis of a sentence with a random mask on a word of a sentence.\n",
    "\n",
    "> BERT uses a technique called `WordPiece` tokenization method to the input. It also uses learned positional encoding. Not sine/cosine approach as in original transformer.\n",
    "\n",
    "For example take the below sentence.\n",
    "\n",
    "    You cannot pass. I am a servant of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "BERT would mask this like below\n",
    "\n",
    "    You cannot pass. I am a [MASK] of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "\n",
    "With this type of input, multi attention sub layer can now try to predict the masked token. (kinda like word2vec method skipgram)\n",
    "\n",
    "There are some tricks used during training to force the model to train longer while producing better results. \n",
    "\n",
    "- Surprising the model by not masking anything for 10% of the dataset.</br>\n",
    "    `The dark fire will not [avail] you, flame of Udun.`\n",
    "- Surprise the model by replacing the masking token with a random token 10% of time.</br>\n",
    "    `The dark fire will not [dance] you, flame of Udun.`\n",
    "- Rest of the data mask by a special token.</br>\n",
    "    `The dark fire will not [MASK] you, flame of Udun.`\n",
    "\n",
    "These weird approches helped the model to avoid overfitting while making the model training efficient.\n",
    "\n",
    "**Next Sentence Prediction**\n",
    "\n",
    "In this technique, the input privided with 2 sentences and 2 new tokens were added.\n",
    "\n",
    "1. **\\[CLS]** : A binary classification like token that get added to the beginning of a sentence to predict if the second sequence follows the first sequence. (ie: posive sample if two sentences are actually consecutive. If sentences are not consecutive then negative)\n",
    "\n",
    "2. **\\[SEP]** A token to indicate the separation/end of a sentence.\n",
    "\n",
    "Example usage of above would be like below.\n",
    "\n",
    "The input sentence\n",
    "\n",
    "    You cannot pass. I am a servant of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "BERT input\n",
    "\n",
    "    [CLS] You cannot pass [SEP] I am a servant of the Secret Fire, wielder of the flame of Anor [SEP]\n",
    "\n",
    "\n",
    "As we can probably guess this require the sentences to embedded differently to make sure that encoding include addtional information to distiquish 2 sequences. Check the below image extracted from the reference book.\n",
    "\n",
    "<center><image src=\"imgs/11.jpg\" width=\"500\"/></center>\n",
    "\n",
    "BERT input embeddings are obtained by summing up 3 types of embeddings token embeddings, sentence sengment embeddings and positional encodings. In additional precise manner, the process of preparing input embeddings for BERT is as follows.\n",
    "\n",
    "- Sequence of text is broken into tokens using `WordPiece` technique.\n",
    "- A \\[MASK] token will replace a random token for Masked Language Model training.\n",
    "- A \\[CLS] classification token is inserted at the beginning of the sequence for classification purposes.\n",
    "- \\[SEP] token is used to indicate the separation/end of sequences for NSP training.\n",
    "- Sentence embedding is added to token embedding, so that 2 sentence sequences have different embedding values.\n",
    "- Positional encoding is learned through the model.\n",
    "\n",
    "Some other key features of BERT are,\n",
    "\n",
    "- It has both unsupervised and supervised training elements.\n",
    "- Bidirectional attention is used in all the attention layers.\n",
    "\n",
    "> Above only describes very high level differences and mechanisms of the BERT model. It is recommended to read more detailed explanations to see the actual implementation details and complexities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
