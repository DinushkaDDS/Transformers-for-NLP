{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Models and Usages\n",
    "\n",
    "Earlier we discussed about the original transformer architecture and what it consists of. We can change the way components inside a transformer interact with each other and build different model. And in the same way we can add new components as well. That is what exactly Bert models do. They have introduced a new component name `Bidirectional multihead attention` sub layer. This helps the new transformer architecture to look at all the words at the same time.\n",
    "\n",
    "BERT or `Bidirectional Encoder Representations from Transformers` only uses blocks from the encoder stack in the original transformer and does not even have a decoder stack. \n",
    "\n",
    "### **Architecture of the BERT model**\n",
    "\n",
    "As I said earlier BERT introduced bidirectional attention to the transformer models. It require many changes to the original Transformer model. Below are the such changes that can be found in BERT models compared to what we discussed in the original transformer.\n",
    "\n",
    "\n",
    "#### BERT Encoder stack\n",
    "The BERT model does not use a decoder layers. It also does the masking of tokens but on the attention layer of the encoder. Also original transformer model had 6 encoder layer stacks with 512 size dimensionality. It's attention layer has 8 heads as well.\n",
    "\n",
    "But in the BERT models it is different.\n",
    "\n",
    "- **BERT base model:** This contained 12 encoder layers with 768 dimensionality. Also attention layer had 12 heads to make each of them get 64 size.\n",
    "\n",
    "- **BERT large model:** This contained 24 encoder layer stack with 1024 dimensionality. Also attention layer had 16 heads to make each of them get 64 size.\n",
    "\n",
    "And due to these large number of dimension sizes and layer counts, BERT models have high number of parameters to train as well compared to the original transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another difference is the masked attention. BERT model researchers found out that having a mask in the attention layer can actually impede the attention process.To mitigate this issue BERT authors came up with new idea. That is pretraining the model with different approach/objective!\n",
    "\n",
    "Practically speaking it means BERT model can be trained with 2 approaches.\n",
    "\n",
    "1. Masked Language Modelling (MLM)\n",
    "2. Next Sentence Prediction (NSP)\n",
    "\n",
    "**Masked Language Modelling**\n",
    "\n",
    "In this task, instead of training a model with sequence of visible words followed by a masked sequence to predict, BERT introduces a bidirectional analysis of a sentence with a random mask on a word of a sentence.\n",
    "\n",
    "> BERT uses a technique called `WordPiece` tokenization method to the input. It also uses learned positional encoding. Not sine/cosine approach as in original transformer.\n",
    "\n",
    "For example take the below sentence.\n",
    "\n",
    "    You cannot pass. I am a servant of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "BERT would mask this like below\n",
    "\n",
    "    You cannot pass. I am a [MASK] of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "\n",
    "With this type of input, multi attention sub layer can now try to predict the masked token. (kinda like word2vec method skipgram)\n",
    "\n",
    "There are some tricks used during training to force the model to train longer while producing better results. \n",
    "\n",
    "- Surprising the model by not masking anything for 10% of the dataset.</br>\n",
    "    `The dark fire will not [avail] you, flame of Udun.`\n",
    "- Surprise the model by replacing the masking token with a random token 10% of time.</br>\n",
    "    `The dark fire will not [dance] you, flame of Udun.`\n",
    "- Rest of the data mask by a special token.</br>\n",
    "    `The dark fire will not [MASK] you, flame of Udun.`\n",
    "\n",
    "These weird approches helped the model to avoid overfitting while making the model training efficient.\n",
    "\n",
    "**Next Sentence Prediction**\n",
    "\n",
    "In this technique, the input privided with 2 sentences and 2 new tokens were added.\n",
    "\n",
    "1. **\\[CLS]** : A binary classification like token that get added to the beginning of a sentence to predict if the second sequence follows the first sequence. (ie: posive sample if two sentences are actually consecutive. If sentences are not consecutive then negative)\n",
    "\n",
    "2. **\\[SEP]** A token to indicate the separation/end of a sentence.\n",
    "\n",
    "Example usage of above would be like below.\n",
    "\n",
    "The input sentence\n",
    "\n",
    "    You cannot pass. I am a servant of the Secret Fire, wielder of the flame of Anor.\n",
    "\n",
    "BERT input\n",
    "\n",
    "    [CLS] You cannot pass [SEP] I am a servant of the Secret Fire, wielder of the flame of Anor [SEP]\n",
    "\n",
    "\n",
    "As we can probably guess this require the sentences to embedded differently to make sure that encoding include addtional information to distiquish 2 sequences. Check the below image extracted from the reference book.\n",
    "\n",
    "<center><image src=\"imgs/11.jpg\" width=\"500\"/></center>\n",
    "\n",
    "BERT input embeddings are obtained by summing up 3 types of embeddings token embeddings, sentence sengment embeddings and positional encodings. In additional precise manner, the process of preparing input embeddings for BERT is as follows.\n",
    "\n",
    "- Sequence of text is broken into tokens using `WordPiece` technique.\n",
    "- A \\[MASK] token will replace a random token for Masked Language Model training.\n",
    "- A \\[CLS] classification token is inserted at the beginning of the sequence for classification purposes.\n",
    "- \\[SEP] token is used to indicate the separation/end of sequences for NSP training.\n",
    "- Sentence embedding is added to token embedding, so that 2 sentence sequences have different embedding values.\n",
    "- Positional encoding is learned through the model.\n",
    "\n",
    "Some other key features of BERT are,\n",
    "\n",
    "- It has both unsupervised and supervised training elements.\n",
    "- Bidirectional attention is used in all the attention layers.\n",
    "\n",
    "> Above only describes very high level differences and mechanisms of the BERT model. It is recommended to read more detailed explanations to see the actual implementation details and complexities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model training framework\n",
    "\n",
    "\n",
    "BERT is a two step framework. It invloves a pretraining step and then a finetuning step.\n",
    "\n",
    "<center><image src=\"imgs/12.png\" width=\"700\"/></center>\n",
    "\n",
    "\n",
    "In the pretraining part, we need to define the model architecture, number of layers, number of heads, dimensions and other things. Then once a base model defined it will get trained with very large amount of data like wikipedia data, Bookcorpus etc using the techniques we mentioned like MLM and NSP.\n",
    "\n",
    "Then the model that got pretrained will be tuned to the required task/objective with more specialized labeled datasets. These downstream specialized task include Natural Language understanding , Question answering and Adversarial Generation Sentence-Pairs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Fine Tuning Example\n",
    "\n",
    "Below we will try to finetune a bert model for a task called Neural Network Acceptability Judgement. Basically we are trying to train a BERT model to identify whether a given sentence is grammatically accurate. To do that we will use the Corpus of Linguistic Acceptability(CoLA).\n",
    "\n",
    "> Using a GPU is essential, so using google colab is recommended. Anyway code is here use at you own risk!\n",
    "\n",
    "First thing we need to do is checking whether the GPU is accessible to the framework we are using. We can check it like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to install the BERT pretrained model To fo that, we will use the hugging face transformers python package. We can install the package by `pip install transformers` command.\n",
    "\n",
    "Once it is completed we can import the modules we will require for the task. Specially pretrained modules, pretrained BERT tokenizer and configurations of BERT model along with BERTAdam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then since we are trying to use the pytorch based BERT model, we need to specify pytorch to use CUDA. So we need to get the device variable in hand like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to work, related data files are in the data folder. We can observe the data set using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our friends wo n't buy this analysis , let alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one more pseudo generalization and i 'm giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one more pseudo generalization or i 'm giving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the more we study verbs , the crazier they get .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>day by day the facts are getting murkier .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>poseidon appears to own a dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>digitize is my happiest memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it is easy to slay the gorgon .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i had the strangest feeling that i knew you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what all did you get for christmas ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8551 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "0               gj04      1         NaN   \n",
       "1               gj04      1         NaN   \n",
       "2               gj04      1         NaN   \n",
       "3               gj04      1         NaN   \n",
       "4               gj04      1         NaN   \n",
       "...              ...    ...         ...   \n",
       "8546            ad03      0           *   \n",
       "8547            ad03      0           *   \n",
       "8548            ad03      1         NaN   \n",
       "8549            ad03      1         NaN   \n",
       "8550            ad03      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "0     our friends wo n't buy this analysis , let alo...  \n",
       "1     one more pseudo generalization and i 'm giving...  \n",
       "2     one more pseudo generalization or i 'm giving ...  \n",
       "3      the more we study verbs , the crazier they get .  \n",
       "4            day by day the facts are getting murkier .  \n",
       "...                                                 ...  \n",
       "8546                   poseidon appears to own a dragon  \n",
       "8547                     digitize is my happiest memory  \n",
       "8548                    it is easy to slay the gorgon .  \n",
       "8549      i had the strangest feeling that i knew you .  \n",
       "8550               what all did you get for christmas ?  \n",
       "\n",
       "[8551 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['sentence_source', 'label', 'label_notes', 'sentence']\n",
    "input_data = pd.read_csv('data/in_domain.tsv', sep='\\t', header=None, names=col_names)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label 1 implies sentence is grammatically make sense. Otherwise it is not.\n",
    "\n",
    "Now lets prepare the dataset sentences as described earlier in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = input_data.sentence.values\n",
    "\n",
    "# Here we are adding the CLS and SEP tokens to the sentence beginning and end.\n",
    "sentences = ['[CLS] ' + s + ' [SEP]' for s in sentences]\n",
    "labels = input_data.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can send this prepared sentences to the pretrained BERT tokenizer (It is called pretrained but basically it means tokens get assigned a unique ID related to some algorithms in BERTS case wordpiece. FTW!). But as you can probably understand this is a time consuming task to generate from scratch. So we are using a precalculated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'one',\n",
       " 'more',\n",
       " 'pseudo',\n",
       " 'general',\n",
       " '##ization',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'giving',\n",
       " 'up',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(s) for s in sentences]\n",
    "tokenized_texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tokenization, we need to process our data sequences in to same size. In the original paper authors have used size 512, But our dataset is small and the maximum sequence size we have is 47. So we will use 128 instead.\n",
    "\n",
    "If anyone of you was confused why this is needed, it is not because the model cant handle varying lengths of sequence sizes. But because of the way data batches get trained during parrallel processing. To do the matrix computations per batch we need to have similar dimensions for all the elements inside of it, hence the padding for sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2256, 2814, ...,    0,    0,    0],\n",
       "       [ 101, 2028, 2062, ...,    0,    0,    0],\n",
       "       [ 101, 2028, 2062, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 2009, 2003, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2018, ...,    0,    0,    0],\n",
       "       [ 101, 2054, 2035, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', \n",
    "                    truncating='post', padding='post')\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes an interesting part. We added the padding. But it is just for computational efficiency purposes. So we need to make sure padded values does not get included in any calculations. To do that we can use a masking matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets divide the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2022, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(masks, input_ids,random_state=2022, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in a separated state. But to use it in pytorch based model, we need to convert it to a tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have the data in the desired format, we cant use it directly due to various resource constraints. So we need to have a data batching mechanism to load data. Each framework have different methods to do this. Check the corresponding documentations.\n",
    "\n",
    "In our case we simply convert the data to torch Dataloader class, with batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, sampler=validation_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready with the data. Now we can focus on the model itself. We will be using the huggingface transformers package to get BERT model with 'bert-base-uncased' style config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the BERT config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above configuration show the base config for BERT model we are trying to fine tune.\n",
    "\n",
    "Here most of the configurations are straightforward, So I am not going to explain. But the details of some confusing ones are as follows.\n",
    "\n",
    "- initializer_range: \n",
    "    This denotes the initializing standard deviation of the weight matrices.\n",
    "\n",
    "- intermediate_size:\n",
    "    The dimension of the feed forward network inside the encoder.\n",
    "\n",
    "- type_vocab_size:\n",
    "    Denotes the number of token type ids which used to identify the sequences. Basically if we are having 2 sequences separated by \\[SEP] we would have 2 token type ids.\n",
    "\n",
    "Now lets load the BERT model itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code gives us the overview of the BERT model architecture. Below is a part of it.\n",
    "\n",
    "<center><image src=\"imgs/13.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we prepared the data, loaded the model and loaded the configuations. Now we need to load the optimizer required by the model to do the actual training. Apparently BERT has used a weight decay rate technique to avoid overfitting and to maintain stability. But it has not applied to all parameters.\n",
    "\n",
    "Below code to get those values. I directly copied it from the source code, as I have no clue what exactly it tries to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is taken from: Transformers for NLP book source code.\n",
    "# Derived source: https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
    "\n",
    "# Don't apply weight decay to any parameters whose names include these tokens.\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# Separate the 'weight' parameters from the 'bias' parameters. \n",
    "# - For the 'weight' parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
    "# - For the 'bias' parameters, the 'weight_decay_rate' is 0.0. \n",
    "optimizer_grouped_parameters = [\n",
    "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.1},\n",
    "    \n",
    "    # Filter for parameters which *do* include those.\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
