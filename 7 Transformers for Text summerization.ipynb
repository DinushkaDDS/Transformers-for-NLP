{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Text Summerization\n",
    "\n",
    "As we discussed earlier transformers are great for many NLP tasks and text summerization is one such task.\n",
    "\n",
    "In 2019, a team of researchers formed a new model based on the assertion every NLP problem can be represented as a Text-To-Text function(and if we in fact think about it, most of the tasks we expect a transformer to do falls in to such category). The idea was to train a transformer using transfer learning techniques during the training phase and finetune using text to text approach. \n",
    "\n",
    "This idea led to new performance level for transformer models and the model is called Text-To-Text Transfer Transformer or T5 for short. The researchers wanted the T5 model to be have task agnostic training process. To acheive that purpose they simply added a prefix to the sequences defining what need to be done. For example,\n",
    "\n",
    "- 'translate English to German: \\[sequence]'\n",
    "- 'summerize: \\[sequence]'\n",
    "\n",
    "This way T5 models can get different tasks in single format. This led T5 to be used in many use cases with same parameters.\n",
    "\n",
    "### T5 Architecture\n",
    "\n",
    "As mentioned earlier T5 researchers was not interested in finding a new transformer architecture. Instead they were interested in making the transformer input agnostic. Therefore they used the original transformer architecture for their purpose. But they slightly changed few functionalities to match their need.\n",
    "\n",
    "- T5 self attention is `order independent`. Means works on a set rather that a list of sequential tokens like previous models.\n",
    "\n",
    "- Instead of using positional embedding technique of original transformer, this uses a relative postional embedding technique. Also usage of this embeddings are bit different compared to previous models.\n",
    "\n",
    "\n",
    "> Read more descriptive details about the T5 model for better explanations.\n",
    "\n",
    "***\n",
    "\n",
    "### Text summerization with T5\n",
    "\n",
    "We will use huggingface provided T5 model for this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Config, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # can use gpu if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
