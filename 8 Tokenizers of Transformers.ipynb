{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers of Transformers\n",
    "\n",
    "After the initial introduction of the original transformer, it evolved so quickly within short amount of time. With each of those iterations, transformers changed the way it process the input data. Hence it is important to understand the preprocessing need to be done to each of the model. \n",
    "\n",
    "One important aspect of this preprocessing is tokenization. Lets say we used a tokenizer trained on a generic news articles to process some advance science article may be biology. In that case there's very high chance the tokenizer will do very bad things to the tokens we need. So it is important to choose the right type of tokenizer for our usecase/datasets.\n",
    "\n",
    "First we will look at the latest preprocessing best practices. These are found to be effective when training transformers.\n",
    "\n",
    "\n",
    "1. Only choose sentences with punctuation marks. (If we need to teach a machine about our language, we first show it what is proper language.)\n",
    "2. Remove bad words. (this is due to obvious reasons)\n",
    "3. Remove code (this may depend on the task we are doing, but basically removing numerical components is a good idea.)\n",
    "4. Check Language (in somecases datasets may contain multiple languages in one sections. This may cause problems.)\n",
    "5. Grammatical check (some online datasets may contain sentences that makes no sense. It is better to filter out such data.)\n",
    "\n",
    "\n",
    "Other than above in commercial applications it is better to check the data for any discrimanations, bad informations etc and remove them. Otherwise it may cause problems down the line from ethical and legal perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ddsdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open('data/text.txt', 'r', encoding='utf8')\n",
    "raw_data = myfile.read()\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['december', ',', '1971', '[', 'etext', '#', '1', ']', 'the', 'project', 'gutenberg', 'etext', 'of', 'the', 'declaration', 'of', 'independence', '.'], ['all', 'of', 'the', 'original', 'project', 'gutenberg', 'etexts', 'from', 'the', '1970', \"'s\", 'were', 'produced', 'in', 'all', 'caps', ',', 'no', 'lower', 'case', '.'], ['the', 'computers', 'we', 'used', 'then', 'did', \"n't\", 'have', 'lower', 'case', 'at', 'all', '.']]\n"
     ]
    }
   ],
   "source": [
    "data = raw_data.replace('\\n', ' ')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for sent in sent_tokenize(data):\n",
    "    words = []\n",
    "\n",
    "    for word in word_tokenize(sent):\n",
    "        words.append(word.lower())\n",
    "    sentences.append(words)\n",
    "\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=11806, vector_size=512, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=512, window=5, sg=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is Word2Vector model with skipgram training. It has 512 size word embeddings after the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(model, word1, word2):\n",
    "    \n",
    "    try:\n",
    "        a = model.wv[word1]\n",
    "        b = model.wv[word2]\n",
    "        \n",
    "        a = a.reshape(1, 512)\n",
    "        b = b.reshape(1, 512)\n",
    "\n",
    "        return cosine_similarity(a, b)\n",
    "\n",
    "    except:\n",
    "        print(\"OOV word!\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity [[0.3650815]] freedom liberty\n"
     ]
    }
   ],
   "source": [
    "word1=\"freedom\"\n",
    "word2=\"liberty\"\n",
    "\n",
    "print(\"Similarity\", similarity(model, word1, word2), word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV word!\n",
      "Similarity None cooperations rights\n"
     ]
    }
   ],
   "source": [
    "word1=\"cooperations\"\n",
    "word2=\"rights\"\n",
    "\n",
    "print(\"Similarity\", similarity(model, word1, word2), word1, word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case where the considering word is not in the vocabulary, some models will not be able to provide a result. Like in above case. This may cause chain of issues if not handled properly.\n",
    "\n",
    "Also on the other hand, if we used a tokenizer like Byte Pair tokenizer it may split the words to segments that may yield different results than the original meaning of the word. So it is important to have a human intervined quality controlling method in many of the critical decision making systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity [[0.20786054]] books ebooks\n"
     ]
    }
   ],
   "source": [
    "word1=\"books\"\n",
    "word2=\"ebooks\"\n",
    "\n",
    "print(\"Similarity\", similarity(model, word1, word2), word1, word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the above 2 words. They should be similar, at least to some extent. But in this case, it is not the case. This can happen to many reasons including the way these words have been used in the dataset, rararity of the considering word or purely because of some random noise. \n",
    "\n",
    "So it is important to make sure such issues are properly tested in rigourous manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity [[0.49995035]] pay debt\n"
     ]
    }
   ],
   "source": [
    "word1=\"pay\"\n",
    "word2=\"debt\"\n",
    "\n",
    "print(\"Similarity\", similarity(model, word1, word2), word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
