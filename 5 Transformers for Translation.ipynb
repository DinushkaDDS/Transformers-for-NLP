{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation using Transformers\n",
    "\n",
    "Language translation is one of the most important task in NLP world. As people get connected with each other, language translation helps them to understand each other better. \n",
    "\n",
    "Machine translation can formally be identified as the process of reproducing human translation using machine transductions(Transduction means taking structures we perceive and representing them with another form).\n",
    "\n",
    "And in real life, this is a complicated task due to many reasons. For example lets say there's a sentence A which is written in some language. Then we are required to translate this sentence in to another language. A human translator comes and translate the sentence A to give an translated output. But would another human translator give a same translation to the sentence A? This is probably not! But the problem is that both translated sentences would have same meaning. So when we try to build a model to do translation, our model should be able to understand/perform the same.\n",
    "\n",
    "To see how such task is achieved we will look at a task of tranlating French/English. We will be using the WMT(Workshop of Machine Translation) dataset provided in [this url](https://www.statmt.org/europarl/v7/fr-en.tgz).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_scripts.preprocess import *\n",
    "import pickle\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we clean up the input data for English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data('data/translation_data/europarl-v7.fr-en.en')\n",
    "sentences = get_sentence_list(raw_data)\n",
    "min_len, max_len = get_sentence_size_range(sentences)\n",
    "cleaned_sentences = clean_sentences(sentences)\n",
    "\n",
    "outfile = open('data/translation_data/English.pkl','wb')\n",
    "pickle.dump(cleaned_sentences, outfile)\n",
    "outfile.close()\n",
    "print(\"data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same process for the French data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data('data/translation_data/europarl-v7.fr-en.fr')\n",
    "sentences = get_sentence_list(raw_data)\n",
    "min_len, max_len = get_sentence_size_range(sentences)\n",
    "cleaned_sentences = clean_sentences(sentences)\n",
    "outfile = open('data/translation_data/French.pkl','wb')\n",
    "pickle.dump(cleaned_sentences, outfile)\n",
    "outfile.close()\n",
    "print(\"data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed the basic preprocessing. But this is not enough for NLP tasks. We need to do several other tasks as removing unnecessary tokens (very rare ones), get vocabulary and word counts etc. as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_scripts.further_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is saved to disk!\n"
     ]
    }
   ],
   "source": [
    "english_data = load_cleaned_data('data/translation_data/English.pkl')\n",
    "vocab = get_vocabulary(english_data)\n",
    "trimmed_vocab = trim_vocabulary(vocab)\n",
    "data = update_dataset(english_data, trimmed_vocab)\n",
    "save_processed_data(data, 'data/translation_data/english_cleaned_sentences.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is saved to disk!\n"
     ]
    }
   ],
   "source": [
    "french_data = load_cleaned_data('data/translation_data/French.pkl')\n",
    "vocab = get_vocabulary(french_data)\n",
    "trimmed_vocab = trim_vocabulary(vocab)\n",
    "data = update_dataset(french_data, trimmed_vocab)\n",
    "save_processed_data(data, 'data/translation_data/french_cleaned_sentences.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now data is ready to be plugged in to a transformer model. Idea is French dataset contains the sentences to translate. English dataset has the reference translations that can be used by model. And model should be able to translate french to english based on the given data.\n",
    "\n",
    "But how can we measure the performance of our translation. As we discussed earlier human language translations for single sentence may have different forms. To help with that BLEU method was introduced.\n",
    "\n",
    "#### BLEU Method\n",
    "\n",
    "Bilingual Evaluation Understudy Score or BLEU is a metric used in evaluating human translation to compensate for the issues we disscussed earlier. BLEU method consider the N-gram tokens of the considering sentences along with few other metrics to evaluate the sentence translation tasks. Check out this [excellent blog post](https://jaketae.github.io/study/bleu/).\n",
    "\n",
    "We will use the NLTK package provided BLEU method for some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 1.0\n",
      "Example 2 1.0\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "reference = [['the', 'cat', 'likes', 'milk'], ['cat', 'likes' 'milk']]\n",
    "candidate = ['the', 'cat', 'likes', 'milk']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print('Example 1', score)\n",
    "\n",
    "#Example 2\n",
    "reference = [['the', 'cat', 'likes', 'milk']]\n",
    "candidate = ['the', 'cat', 'likes', 'milk']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print('Example 2', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above is fine, but there are cases when the unigram based precision may give wrong results. In such cases NLTK package will complain/warn to use a smoothing function.\n",
    "\n",
    "These Smoothing functions helps in cases where tokens are not available in the candidate sentences.  This is not complicated, but I am not particularly interested in studying this for the moment, So will leave it there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "So far we looked at data preparation, and simple evaluation methods of translation problems. Now lets move on to a model training process for a translation problem. We will use a library called `Trax` from google for this. We will not be building a model from scratch, but will use a provided weights to initialize our model.\n",
    "\n",
    "For more details about Trax visit [Trax Home Page](https://trax-ml.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "Unfortunately Windows does not suppor JaxLib which is a dependency for the trax installation, so you will need to run the code in a linux based environment. (Or WSL!)\n",
    "\n",
    "Check out this [Notebook](https://github.com/PacktPublishing/Transformers-for-Natural-Language-Processing/blob/main/Chapter05/Trax_Translation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
