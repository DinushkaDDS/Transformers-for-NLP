{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - How things came to here!\n",
    "\n",
    "Several years back, before deep learning taking a massive outbreak most of the natural language processing tasks were stuck at a very primitive level. After the DL initial usages, RNNs and CNNs came into the scene and improved the models we used to do the NLP tasks greatly. But none of them affected the NLP world as the Transformers architecture which was introduced by a google researcher team.\n",
    "\n",
    "In early days of NLP, Markov decision processes, markov models, chains were used to predict the next element of a sequence given the past elements. Then these turned into the space of CNNs. But CNNs have the drawback of not keeping track of sequences in long/complex inputs. Then came the RNNs and LSTMs to mitigate the above issues which can keep a state within. \n",
    "\n",
    "After all those, in 2017 google research/brain scientists published a paper `Attention is all you need` which introduces the architecture of Transformers. \n",
    "\n",
    "For the moment I will not go in to the details of the model architecture of the original transformer. But important point to know is attention mechanism replaced the recurrence which need to have high computational power to keep details of long sequences. Instead attention is a `word to word` operation. The attention mechanism will find how each word is related to each other including itself. Therefore it can provide deeper relationships between words and produce better results.\n",
    "\n",
    "<center><image src=\"imgs/1.png\" width=\"400\"/></center>\n",
    "\n",
    "Normal Transformer architecture consists with 2 major parts encoder and decoder. Usually these part consists of stacks of layers.\n",
    "\n",
    "### **Encoder stack**\n",
    "\n",
    "The original transformer's encoder stack consists of 6 layers of below stucture.\n",
    "\n",
    "<center><image src=\"imgs/2.jpg\" width=\"200\"/></center>\n",
    "\n",
    "Each of above like structure contains 2 sub layers: a multiheaded attention mechanism and a fully connected(dense) position-wise feedforward network. Also note the residual connections (skip connections) in each such sublayers. This make sure all the key information such as positional encoding is not lost on the way. (Note Embedding is only available to the first one.)\n",
    "\n",
    "#### **Input Embedding**\n",
    "\n",
    "In the original paper this is of size 512. As with any other model embedding layer, usage of this is pretty simple. Assume we have a sentence like below.\n",
    "\n",
    "<center>My name is Dilan and I am a software engineer.</center>\n",
    "\n",
    "Then we need to tokenize it.\n",
    "\n",
    "<center>[My, name, is, Dilan, and, I, am, a, software, engineer, .]</center>\n",
    "\n",
    "Since we cant use text in processes, we will assign integer values for each token.\n",
    "\n",
    "<center>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</center>\n",
    "\n",
    "Then using these integers we can define a vocabulary and move in to embedding training. To do that we define a (vocabulary size * predefined width) sized matrix as embedding matrix. Then we use it in the first section of NLP model. To see a example implementation check this [Jupyter Notebook of Mine](https://github.com/DinushkaDDS/DL-models-from-Scratch/blob/master/sentence%20embedding.ipynb).\n",
    "\n",
    "We can use such pretrained embedding layer as the input embedding to a transformer as well. But transformers are based on dynamic embeddings. Which means embedding are trained on the fly.\n",
    "\n",
    "One problem in using regular/old methods of word embeddings is that they do not contain any positional data. Therefore some words may be unnecessarily similar mathamatically even though they cant be language wise or vice versa. To solve that problem original transformer model team introduced a new technique called `positional encoding`.\n",
    "\n",
    "\n",
    "##### Positional Encoding\n",
    "\n",
    "Positional Encoding is the method used to provide the position of a word in a sequence. Having additional vectors to keep positional details is not practical as it makes things complicated computational and architectural wise. Instead, we can add positional encoding to the input embeddings.\n",
    "\n",
    "But to do that we need to make the position encodings to the same size as of the word embeddings. As a solution to the mentioned problem they found a way to use sine and cosine values that will remain small yet useful.\n",
    "\n",
    "<center><image src=\"imgs/3.jpg\" width=\"300\"/></center>\n",
    "\n",
    "In here for odd numbers and even numbers researchers have defined 2 formulas. There are some other techniques to do this as well. But this is the one used by the original transformer team. Code implementation of above to get positional embeddings is as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def positional_encodings(considering_pos, vector_width=512):\n",
    "\n",
    "    positional_encoding_vector = np.zeros((vector_width))\n",
    "    for i in range(0, vector_width, 2):\n",
    "\n",
    "        positional_encoding_vector[i] = \\\n",
    "                                math.sin(considering_pos/(10000**((2*i)/vector_width)))\n",
    "        positional_encoding_vector[i+1] = \\\n",
    "                                math.cos(considering_pos/(10000**((2*i)/vector_width)))\n",
    "\n",
    "    return positional_encoding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 3))\n",
    "\n",
    "for i in range(4):\n",
    "    embeddings = positional_encodings((i+1)*5, 512)\n",
    "    ax[i].plot(range(512), embeddings, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can this is a very interesting approach to get a positional encoding for a given position. The above function returns a vector with the same dimension as the word embedding but unique for each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8485685095237376"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "1- cosine(positional_encodings(1), positional_encodings(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860638318260113"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- cosine(positional_encodings(1), positional_encodings(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see higher the distance between the positions, higher the difference would in terms of encodings. So now we have a meaningful way to represent the positional details in vector form.\n",
    "\n",
    "Now to include these details to the actual word embeddings transformers researchers simply added them together. But this causes issues like actual word meanings get diminished compared to the positional value. To mitigate that we can do some kind of transformation before adding them (eg. Squarerooting, weighted adding etc.) Below is an example implementation of the squarerooted adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embeddings(positional_vector, word_vector, vector_width=512):\n",
    "    out = np.array(positional_vector.shape)\n",
    "\n",
    "    for i in range(len(positional_vector)):\n",
    "        out[i] = positional_vector[i] + word_vector[i]*math.sqrt(vector_width)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the value of word vector has been amplified by a constant value so that, its information get more importance compared to the positional vector in general.\n",
    "\n",
    "Those techniques fullfill the task of generating meaningful embedding for the words. And it's output will next move to the `Multi Head Attention` sub layer.\n",
    "\n",
    "\n",
    "#### **Multi Head Attention**\n",
    "\n",
    "The output of the positionally encoded word embedding will come to this part next. Its general structure include a post layer normalization, a skip connection as well.\n",
    "\n",
    "<center><image src=\"imgs/4.jpg\" width=\"300\"/></center>\n",
    "\n",
    "Before going further it is important to understand the idea and architecture behind multi head attention.\n",
    "\n",
    "##### Architecture\n",
    "\n",
    "In simple terms attention mechanism allows an input to interact with each other(including itself) and find out who should it pay more attention to. The outputs from attention mechanisms are those interactions and attention scores.\n",
    "\n",
    "But there's a concern. Model would only get one timestep at a time. Also to check the interactions with each of the other words in would take considerable amount of time. To solve that input embeddings will get divided in to smaller dimensions(if vector width is 512 then can be divided into 8*64 sized vectors). Then we can do the calculations in parallel for each divided dimension. This is where the multihead part is coming to the naming. \n",
    "\n",
    "And there's few processing would get done in the each of those heads to identify how each embedding is related to each other in a given sequence. Ultimately output of each of these heads would be (size of the sequence * divided dim size).\n",
    "\n",
    "For example assume the sequence 'My name is Dilan Dinushka and I am an engineer' and embedding size 4. Then this would yield (9, 4) matrix. If we define 4 heads in attention, then each head would get (9, 1) sized input and should output the same size(not essential though). \n",
    "\n",
    "Then before sending the processed output to next step we need to combine everything together, so the final output from the multihead attention would be Z = (z1, z2, ......., zn) where zi indicate the output from the ith head(in our previous example this final output would be then (9, 4) dimension again as per the original transformer implementation).\n",
    "\n",
    "\n",
    "Now lets move on to the each attention heads inner workings. Inside of the each head of attention mechanism have 3 representations of each word vector passed to it.\n",
    "\n",
    "- **Q** : A Query vector that has a dimension of head_width and get trained when a word vector xi seeks all of the key value pairs of other word vectors.\n",
    "\n",
    "- **K** : A Key vector that has a dimension of head_width which will be trained to give an attention formula value.\n",
    "\n",
    "- **V** : A Value vector that has same dimension as above ones which will be trained to provide another attention formula value.\n",
    "\n",
    "It should be noted that Attention is a function defined as `Scaled Dot-Product Attention` in the original transformer architecture. Its mathematical form is as below.\n",
    "\n",
    "\n",
    "<center><image src=\"imgs/5.jpg\" width=\"300\"/></center>\n",
    "\n",
    "\n",
    "> Vectors having same dimensions help when getting the dot product. Otherwise it ok to define dimensions that would satisfy the dot product constraints. Also this is what was used in the original transformers paper. (And at the moment that is what we are studying)\n",
    "\n",
    "\n",
    "Now how to get the Q, K and V vectors. Thats where the weight matrices Qw, Kw and Vw comes in to play. In original transformer model their dimensions are 64 * 512 (head_width*embedding_size). Anyway the idea is that given the sequence of word vectors (x) getting dot product of it with any of the weight matrices should yield the corresponding representational vector.\n",
    "\n",
    "    x dot_product Qw ==> Q vector\n",
    "\n",
    "    Below are the related dimensional details.\n",
    "\n",
    "    (sequence_length * embedding_size) * (embedding_size * head_width) ==> (sequence_length*head_width)\n",
    "\n",
    "\n",
    "So this way weight matrices can be in fixed size while sequence length(x) sizes change.\n",
    "\n",
    "With the above details in mind, lets look at a basic implementation of attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X is a dataset with vector width(Embedding size) of 4 and sequence size of 3.\n",
    "x = np.array(\n",
    "    [[1, 0, 1, 0],\n",
    "     [0, 2, 0, 2],\n",
    "     [1, 1, 1, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to define the weight matrices Qw, Kw and Vw. Since Embedding size is 4 in our case each of these matrices should have 4 number of rows. We will define the column sizes to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Weight Matrix\n",
    "Qw = np.array([[1, 0, 1],\n",
    "               [1, 0, 0],\n",
    "               [0, 0, 1],\n",
    "               [0, 1, 1]])\n",
    "\n",
    "# Key Weight matrix\n",
    "Kw = np.array([[0, 0, 1],\n",
    "               [1, 1, 0],\n",
    "               [0, 1, 0],\n",
    "               [1, 1, 0]])\n",
    "\n",
    "# Values Weight matrix\n",
    "Vw = np.array([[0, 2, 0],\n",
    "               [0, 3, 0],\n",
    "               [1, 0, 3],\n",
    "               [1, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the weight matrices we can calculate the Q, K and V representations for each word. To do that, we need to get the dot product of input matrix and weight matrices as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.dot(x, Qw)\n",
    "K = np.dot(x, Kw)\n",
    "V = np.dot(x, Vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q, K and V matrices include the respective representations of each word.\n",
    "\n",
    "Then we can calculate the attention value for each of the words compared to each other. In attention value we multiply the query value of one word with each of the Key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[0]@K[0] # This represents the attention value between the first word and its key\n",
    "Q[0]@K[1] # This represents the attention value between the first word and second key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can calculate such attention values for each word query by taking the matrix dot product with Key matrix. (Also here we have scaled it with the dimension of the embedding vecor as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  4.],\n",
       "       [ 4., 16., 12.],\n",
       "       [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_attention = np.dot(Q, K.T)/(int(3**0.5))\n",
    "scaled_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above we can calculate the softmax applied attention for each vector. Below values can be considered as the attention scores among each of the words in the considering sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.33789383e-02, 4.68310531e-01, 4.68310531e-01],\n",
       "       [6.03366485e-06, 9.82007865e-01, 1.79861014e-02],\n",
       "       [2.95387223e-04, 8.80536902e-01, 1.19167711e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "attention_scores = softmax(scaled_attention, axis=1)\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above represent the attention scores for the each of the initial 3 embeddings we define compared to each other.\n",
    "\n",
    "Then comes the place where we need to calculate the values related to each of the inputs. To do that we have to multiply the each of the attention scores with their respective representative Value (V) vector.\n",
    "\n",
    "This is bot confusing compared to previous tasks. So heres a sample example. Assume for a word in our sequence we got attention score vector '[ 1, 2, 4]'. These values represent how much weightage the value representations from the word sequence should provide. Check the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06337894, 0.12675788, 0.19013681],\n",
       "       [0.93662106, 3.74648425, 0.        ],\n",
       "       [0.93662106, 2.80986319, 1.40493159]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_score_w1_w1 = attention_scores[0][0]* V[0]\n",
    "weighted_score_w1_w2 = attention_scores[0][1]* V[1]\n",
    "weighted_score_w1_w3 = attention_scores[0][2]* V[2]\n",
    "weighted_score_w1 = np.stack((weighted_score_w1_w1, \n",
    "                                    weighted_score_w1_w2, \n",
    "                                    weighted_score_w1_w3), axis=0)\n",
    "weighted_score_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above calculations is just for the first vector of the attention score matrix.\n",
    "\n",
    "Then we need to take the element wise summation for the above list of vectors to get the actual output. This yields the attention based output value vector for the selected word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93662106, 6.68310531, 1.59506841])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_w1 = weighted_score_w1.sum(axis=0)\n",
    "output_w1 # This is the output related to the Vector 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAs we mention earlier this is only for one word vector. We need to calculate the weighted_scores and outputs for other vectors in the attention score matrix in the same way. Since manually taking attention weights and multiplying it with the value vectors is time consuming, we can do it in a vectorized manner as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6.33789383e-02, 1.26757877e-01, 1.90136815e-01],\n",
       "        [9.36621062e-01, 3.74648425e+00, 0.00000000e+00],\n",
       "        [9.36621062e-01, 2.80986319e+00, 1.40493159e+00]],\n",
       "\n",
       "       [[6.03366485e-06, 1.20673297e-05, 1.81009946e-05],\n",
       "        [1.96401573e+00, 7.85606292e+00, 0.00000000e+00],\n",
       "        [3.59722029e-02, 1.07916609e-01, 5.39583043e-02]],\n",
       "\n",
       "       [[2.95387223e-04, 5.90774446e-04, 8.86161669e-04],\n",
       "        [1.76107380e+00, 7.04429521e+00, 0.00000000e+00],\n",
       "        [2.38335422e-01, 7.15006266e-01, 3.57503133e-01]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_scores = np.expand_dims(attention_scores, axis=2)*V # Note the elementwise multiplication\n",
    "weighted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.93662106, 6.68310531, 1.59506841],\n",
       "       [1.99999397, 7.9639916 , 0.05397641],\n",
       "       [1.99970461, 7.75989225, 0.35838929]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = weighted_scores.sum(axis=1)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the combinations of everything we did so far in one function.\n",
    "\n",
    "Also note that this is one mechanism of calculating the attention values. Instead of calculating weighed scores sum, we can directly multiply the attention_scores with Value(V) matrix as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.93662106, 6.68310531, 1.59506841],\n",
       "       [1.99999397, 7.9639916 , 0.05397641],\n",
       "       [1.99970461, 7.75989225, 0.35838929]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_attention(Q, W, V, k_d):\n",
    "    '''\n",
    "    Here Q, W and V are matrices that comes after multiplication with\n",
    "    input sequence matrix.\n",
    "\n",
    "    k_d = embedding width\n",
    "    '''\n",
    "    temp = np.dot(Q, K.T)/(int(k_d**0.5))\n",
    "    attention_scores = softmax(temp, axis=1)\n",
    "    weighted_scores = np.expand_dims(attention_scores, axis=2)*V\n",
    "    attention_output = weighted_scores.sum(axis=1)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "att_val = calc_attention(Q, K, V, 3)\n",
    "att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a wrap up remember we had a input sequence with length 3, and each had size 4 embedding (So 3x4 matrix). Above reflect what happen to this input matrix inside of a attention layer (more specifically in the original transformer attention layer).\n",
    "\n",
    ">There are several ways to calculate attention other than the one we used here. Read about them if interested!\n",
    "\n",
    "Now if we go back to where we started this mess, we were discussing the attention mechanism in the original transformers. And we said that it have some special technique called multiheaded attention which basically means input get splitted from its embedding dimension and attention operation will get applied separetely. So how the attention applied values get combined after the split?\n",
    "\n",
    "To see this from here onwards lets assume that we are having 3x64 sized outputs from each heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random values to represent the output values from 8 attention heads.\n",
    "head_out1 = np.random.random((3, 64))\n",
    "head_out2 = np.random.random((3, 64))\n",
    "head_out3 = np.random.random((3, 64))\n",
    "head_out4 = np.random.random((3, 64))\n",
    "head_out5 = np.random.random((3, 64))\n",
    "head_out6 = np.random.random((3, 64))\n",
    "head_out7 = np.random.random((3, 64))\n",
    "head_out8 = np.random.random((3, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming above are the outputs from 8 attention heads we can combine them by concatenating them. But in the original implementation there's a weight matrix that get added to the concataneted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50797615, 0.96253788, 0.40681692, ..., 0.56499623, 0.57256347,\n",
       "        0.55260343],\n",
       "       [0.56447982, 0.90030537, 0.02743779, ..., 0.66184638, 0.60194478,\n",
       "        0.54083157],\n",
       "       [0.07001466, 0.48846472, 0.05417758, ..., 0.1667208 , 0.32225329,\n",
       "        0.58818299]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output = np.hstack((head_out1,\n",
    "                                  head_out2,\n",
    "                                  head_out3,\n",
    "                                  head_out4,\n",
    "                                  head_out5,\n",
    "                                  head_out6,\n",
    "                                  head_out7,\n",
    "                                  head_out8))\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the operations related to attention block is now done! \n",
    "\n",
    "Then this output will be moved to post-layer normalization.\n",
    "\n",
    "\n",
    "#### **Post Layer Normalization (Post-LN)**\n",
    "\n",
    "\n",
    "The output from the attention layer will then move in to here.\n",
    "\n",
    "<center><image src=\"imgs/6.jpg\" width=\"200\"/></center>\n",
    "\n",
    "Basically what this sub block does this adding the residual connection coming from the beginning of the sublayer (in this case before the attention part. Check the diagram for clarity) to the output of the attention part.\n",
    "\n",
    "        LayerNorm( x + Sublayer(x))\n",
    "\n",
    "Here sublayer part refers the output of the sublayer.\n",
    "\n",
    "When considering the normalization methods, there are many and it changes from model to model. But basic one can be defined as below.\n",
    "\n",
    "<center><image src=\"imgs/7.jpg\" width=\"300\"/></center>\n",
    "\n",
    "        Here, \n",
    "                μ = mean\n",
    "                σ = standard deviation\n",
    "                γ = scaling parameter\n",
    "                β = bias vector\n",
    "\n",
    "Once we do the normalization then the all the processing related to the first sub layer will be completed. Then comes the second sub layer of the attention module.\n",
    "\n",
    "#### **Feedforward Network**\n",
    "\n",
    "The input to this sub section is the output from the normalization layer.\n",
    "\n",
    "<center><image src=\"imgs/8.jpg\" width=\"200\"/></center>\n",
    "\n",
    "The Feedforward network in both encoder and decoder stacks are straightforward compared to the attention parts.\n",
    "\n",
    "        They are fully connected.\n",
    "        It contains 2 layers and applies a ReLu funtions.\n",
    "        Input/ Output is 512, hidden layer contain 2048 neurons.\n",
    "\n",
    "Output from the FF Network then goes through a Post LN part as before. Then output can either go to another layer of encoder stack or to the decoder stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder Stack**\n",
    "\n",
    "Decoder part of transformer network contains stacks of layers same as the encoder. Each of such layer would have the following structure.\n",
    "\n",
    "\n",
    "<center><image src=\"imgs/9.jpg\" width=\"200\"/></center>\n",
    "\n",
    "\n",
    "As we can see each layer of the decoder contains 3 sublayers. Feed forward, Multihead Attention and Masked Multihead Attention are those sublayers.\n",
    "\n",
    "Out of those 3, first 2 feedforward and multihead attention sublayers are same as the encoder stack. But the third one is bit different.\n",
    "\n",
    "Masked Multihead attention layer has special function where from a given position rest of the word embeddings will get masked. This causes the model to base it's inference without seeing the complete sequence.\n",
    "\n",
    "Also just like the encoder stack there are residual(skip) connections surrounding the 3 sublayers as well. Other than that, all the sublayer outputs including the emebedding layer output have the same dimension size.\n",
    "\n",
    "Since most of the parts are similar we will not discuss more about those parts. Instead we will focus on the differences in the decoder compared to the encoder stack.\n",
    "\n",
    "\n",
    "> The current transformer model we are discussing is used for language translations task. And therefore we will discuss the transformer functionality in that sense.\n",
    "\n",
    "#### **Output Embeddings & Positional Encodings**\n",
    "\n",
    "As initial transformer was used in a language translational task, its expected output is another language. So that part also needed to be embedded before using. And that's where the decoder stack's embedding layer (named Output embedding) comes in to play. It does the exact same thing as the encoder layer along with a similar positional encoding.\n",
    "\n",
    "#### **Attention layers**\n",
    "\n",
    "> Transformer decorders are of an auto-regressive type model. It means it uses its previous outputs as a part of input for the next iterations. \n",
    "\n",
    "The decoder's masked multihead attention layer only let the attention happen to positions up to and including the current cosidering position. Basically it means when calculating the attention, it does not get to see the full sequence (In practical sense not allowing the model to see into future in terms of output). \n",
    "\n",
    "From there onward, multi-head attention layers also only use the positions not masked by the previous layer (means only up to the current position transformer is predicting) to avoid seeing the sequence it should predict ideally.\n",
    "\n",
    "This masking function is simply for making the training process parallel. Otherwise we will need to sequentially run the examples through the decoding network. And infact that's how transformers work in inference phase in some cases.\n",
    "\n",
    "What masking function does is it helps the model to mimic the sequential behaviour while in the training phase by covering the words in the sequence.\n",
    "\n",
    "\n",
    "<center><image src=\"imgs/10.png\" width=\"200\"/></center>\n",
    "\n",
    "\n",
    "So in the decoder attention,\n",
    "\n",
    "    for first sequence input => encoder output\n",
    "    for second sequence input => encoder output + prediction from the first part\n",
    "    .....\n",
    "    \n",
    "so on until the sequence is completed (this is why it is called autoregressive model since predictions in the previous runs are being considered for next runs.)\n",
    "\n",
    "To achieve this we are adding a mask matrix with negative infinity values as outlined in the above image to the scaled scores matrix. This causes the matrix to yield 0 values in require places after the softmax function. This is super interesting technique used in transformers!\n",
    "S\n",
    "For further clarifications about the transformer layers, read these excellent explanations.\n",
    "\n",
    "[Medium Article on Transformers](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n",
    "\n",
    "[Stackoverflow Answer](https://stackoverflow.com/questions/58127059/how-to-understand-masked-multi-head-attention-in-transformer)\n",
    "\n",
    "\n",
    "\n",
    "Another important point is that, this decoder's multihead attention layer uses Key and Value (K, V) values from the encoder and Query (Q) values from the masked multihead attention layer. \n",
    "\n",
    "After those layers it goes to the Feedforward network(FFN), a post layer normalization task and then a linear layer before classification. In NLP this is a very large layer with the size of vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it! Thats is the basic architecture of the original transformer. It is bit complicated when implementing in practice. But it solved lots of problem existed in previous models used for sequencial tasks such as RNNs, GRUs, LSTMs etc.\n",
    "\n",
    "But unless you are a researcher it is highly unlikely you will need to implement a transformer from scratch! And to be honest it is bit complicated and time consuming to implement even using a modern ML framework. Not to mention the computational resources needed and comlexities invloving using multiple GPUs.\n",
    "\n",
    "Instead we can use huggingface transformers for most of our usecases. It is easy and straight forward to use.!\n",
    "\n",
    "    pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Transformers_for_NLP-p2LjLMk5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e3e7ee84f555b55b39fee8996bc10de737aa7ecc18a014a936b6dc451810e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
