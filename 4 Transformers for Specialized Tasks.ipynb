{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Downstream tasks.\n",
    "\n",
    "Transformer's real usages come when we are using them for a more specialized tasks rather than guessing the \\<mask> or next sentence (though this is still very useful). In this notebook we will work on how we can use transformers in several specialized NLP tasks. Also we will look upon performance measuring of transformers in such usecases.\n",
    "\n",
    "Since now we are moving into the applications of Transformers, it is best to first look at various evaluation mechanisms used in NLP tasks. Below are some performance metrics used by the `General Language Understanding Evaluation (GLUE)` and `SuperGLUE` benchmarks for NLP.\n",
    "\n",
    "#### **Accuracy Score**\n",
    "\n",
    "Accuracy is one of the simplest form of evaluation. It simply calculates how many predictions are correct out of all the available examples.\n",
    "\n",
    "#### **F1 Score**\n",
    "\n",
    "Another metric which helps in specially uneven data distribution evaluations. \n",
    "\n",
    "                F1-score= 2 * (precision * recall)/(precision + recall)\n",
    "\n",
    "#### **Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "We used this already in our RoBERTa model evaluation as well. This computes a measurement based on all 4 values true positives, false positives, true negatives and false negatives. Its more useful compared to both accuracy and F1 scores even when the class distritions are different.\n",
    "\n",
    "### Proving a model is state of the art\n",
    "\n",
    "Before we can claim a model is SOTA we need to have 3 main things.\n",
    "\n",
    "    1. A model\n",
    "    2. A defined dataset related task\n",
    "    3. A valid metric\n",
    "\n",
    "So far we have worked on models and few metrics we can use. We can use provided benchmark datasets/tasks as the  2nd point.\n",
    "\n",
    "One such benchmark we can use is SuperGLUE. Also there's an old one named GLUE as well!. \n",
    "\n",
    "The idea behind building the General Language Understanding Evaluation \n",
    "(GLUE) datasets was to show that NLU can be used in wide range of tasks. But with the performance of the new models GLUE became outdated as most of these new models bagan to outperform human baseline. So to set a higher human baseline standard, SuperGLUE was introduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the SuperGLUE benchmark, it consists of 8 selected tasks. Below is a screenshot of the taskset. [Web Site](https://super.gluebenchmark.com/tasks/).\n",
    "\n",
    "<center><image src=\"imgs/15.jpg\" width=\"500\"/></center>\n",
    "\n",
    "As we can see, it provides the task instruction, datasets, software and other resources required to solve the problem. Once a team runs the benchmark if it reaches the leaderboard results will get displayed.\n",
    "\n",
    "For an example task there's Machine thinking measure. Here input would be some kind of premise and based on that, model need to choose most plausible answer for the given question out of given answers. This feels like quite complex task to be done by a machine, but this is already matched/passed by models. Check the leaderboard!\n",
    "\n",
    "Below include some brief details about each of these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COPA task\n",
    "\n",
    "As explained earlier, in this task, thesers a input premise. There's a question asked based on that premise along with multiple answers. The goal is to find the most plausible answer to the asked question based on the input premise.\n",
    "\n",
    "#### BoolQ task\n",
    "\n",
    "In this task a boolean answer is expected to a question which was asked along with a passage. Model should be able provide True or False answer based on the content inside of the passage. \n",
    "\n",
    "#### Commitment bank task\n",
    "\n",
    "This is bit complex task compared to other as this involves a premise and a hypothesis. Based on the premise model should be able to identify whether the hypothesis is neutral, entailment or contradictory.\n",
    "\n",
    "#### MultiRC task\n",
    "\n",
    "Multi Sentence Reading Comprehension or MultiRC task give a text to read by the model and to pick the correct answer from the given possible choices to the provided question. This basically mimic the our exam comprehension like questions.\n",
    "\n",
    "#### ReCoRD task\n",
    "\n",
    "Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) represents another complex task for NLP models. In this task model will be given a input text and a query text which has a placeholder in it. Based on the input text paragraph, model needs to find the entity that would fit in to the query text placeholder.\n",
    "\n",
    "#### RTE task\n",
    "\n",
    "Recognizing Textual Entailment or RTE task makes the model read a premise, examine a hypothesis and then should predict whether the premise is entailed by the hypothesis. This require model to understand the text and use logic to answer.\n",
    "\n",
    "\n",
    "#### WiC task\n",
    "\n",
    "Words in Context allows to test the model's ability to process an ambiguous word. In this task model will have to analyze 2 sentences and determine whether a target word has the same meaning in the both sentences.\n",
    "\n",
    "#### WSC task\n",
    "\n",
    "Winograd Schema Challenge also test the model's ability to disambiguate. Here contains sentences which focus on the slight differences of gender pronouns. For example it will provide sentence with pronouns and then model will have to predict whether the given token refers the given pronoun. Below is an example.\n",
    "\n",
    "    The blue cup was on top of the table until it was broken by the cat.\n",
    "\n",
    "    target token: blue cup\n",
    "    pronoun: it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Pytorch_-_Use_Case_Exploration-FSDBtUdB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1153c933a691d5b7872e50b0da6113c7d23c17547b77ca34e6f51a7318bb0ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
